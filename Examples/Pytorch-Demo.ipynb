{"cells": [{"metadata": {"trusted": true}, "cell_type": "code", "source": "from numpy import vstack\nfrom pandas import read_csv\nfrom sklearn.preprocessing import LabelEncoder\nfrom sklearn.metrics import accuracy_score\nfrom torch.utils.data import Dataset\nfrom torch.utils.data import DataLoader\nfrom torch.utils.data import random_split\nfrom torch import Tensor\nfrom torch.nn import Linear\nfrom torch.nn import ReLU\nfrom torch.nn import Sigmoid\nfrom torch.nn import Module\nfrom torch.optim import SGD\nfrom torch.nn import BCELoss\nfrom torch.nn.init import kaiming_uniform_\nfrom torch.nn.init import xavier_uniform_", "execution_count": 2, "outputs": []}, {"metadata": {}, "cell_type": "markdown", "source": "# Load Data"}, {"metadata": {"trusted": true}, "cell_type": "code", "source": "# dataset definition\nclass CSVDataset(Dataset):\n    # load the dataset\n    def __init__(self, path):\n        # load the csv file as a dataframe\n        df = read_csv(path, header=None)\n        # store the inputs and outputs\n        self.X = df.values[:, :-1]\n        self.y = df.values[:, -1]\n        # ensure input data is floats\n        self.X = self.X.astype('float32')\n        # label encode target and ensure the values are floats\n        self.y = LabelEncoder().fit_transform(self.y)\n        self.y = self.y.astype('float32')\n        self.y = self.y.reshape((len(self.y), 1))\n \n    # number of rows in the dataset\n    def __len__(self):\n        return len(self.X)\n \n    # get a row at an index\n    def __getitem__(self, idx):\n        return [self.X[idx], self.y[idx]]\n \n    # get indexes for train and test rows\n    def get_splits(self, n_test=0.33):\n        # determine sizes\n        test_size = round(n_test * len(self.X))\n        train_size = len(self.X) - test_size\n        # calculate the split\n        return random_split(self, [train_size, test_size])", "execution_count": 3, "outputs": []}, {"metadata": {"trusted": true}, "cell_type": "code", "source": "# prepare the dataset\ndef prepare_data(path):\n    # load the dataset\n    dataset = CSVDataset(path)\n    # calculate split\n    train, test = dataset.get_splits()\n    # prepare data loaders\n    train_dl = DataLoader(train, batch_size=32, shuffle=True)\n    test_dl = DataLoader(test, batch_size=1024, shuffle=False)\n    return train_dl, test_dl", "execution_count": 6, "outputs": []}, {"metadata": {}, "cell_type": "markdown", "source": "# Defining the Model"}, {"metadata": {"trusted": true}, "cell_type": "code", "source": "# model definition\nclass MLP(Module):\n    # define model elements\n    def __init__(self, n_inputs):\n        super(MLP, self).__init__()\n        # input to first hidden layer\n        self.hidden1 = Linear(n_inputs, 10)\n        kaiming_uniform_(self.hidden1.weight, nonlinearity='relu')\n        self.act1 = ReLU()\n        # second hidden layer\n        self.hidden2 = Linear(10, 8)\n        kaiming_uniform_(self.hidden2.weight, nonlinearity='relu')\n        self.act2 = ReLU()\n        # third hidden layer and output\n        self.hidden3 = Linear(8, 1)\n        xavier_uniform_(self.hidden3.weight)\n        self.act3 = Sigmoid()\n \n    # forward propagate input\n    def forward(self, X):\n        # input to first hidden layer\n        X = self.hidden1(X)\n        X = self.act1(X)\n         # second hidden layer\n        X = self.hidden2(X)\n        X = self.act2(X)\n        # third hidden layer and output\n        X = self.hidden3(X)\n        X = self.act3(X)\n        return X", "execution_count": 5, "outputs": []}, {"metadata": {}, "cell_type": "markdown", "source": "# Model Training"}, {"metadata": {"trusted": true}, "cell_type": "code", "source": "# train the model\ndef train_model(train_dl, model):\n    # define the optimization\n    criterion = BCELoss()\n    optimizer = SGD(model.parameters(), lr=0.01, momentum=0.9)\n    # enumerate epochs\n    for epoch in range(100):\n        # enumerate mini batches\n        for i, (inputs, targets) in enumerate(train_dl):\n            # clear the gradients\n            optimizer.zero_grad()\n            # compute the model output\n            yhat = model(inputs)\n            # calculate loss\n            loss = criterion(yhat, targets)\n            # credit assignment\n            loss.backward()\n            # update model weights\n            optimizer.step()", "execution_count": 7, "outputs": []}, {"metadata": {}, "cell_type": "markdown", "source": "# Model Evaluation"}, {"metadata": {"trusted": true}, "cell_type": "code", "source": "# evaluate the model\n#https://machinelearningmastery.com/pytorch-tutorial-develop-deep-learning-models/\n\ndef evaluate_model(test_dl, model):\n    predictions, actuals = list(), list()\n    for i, (inputs, targets) in enumerate(test_dl):\n        # evaluate the model on the test set\n        yhat = model(inputs)\n        # retrieve numpy array\n        yhat = yhat.detach().numpy()\n        actual = targets.numpy()\n        actual = actual.reshape((len(actual), 1))\n        # round to class values\n        yhat = yhat.round()\n        # store\n        predictions.append(yhat)\n        actuals.append(actual)\n    predictions, actuals = vstack(predictions), vstack(actuals)\n    # calculate accuracy\n    acc = accuracy_score(actuals, predictions)\n    return acc", "execution_count": 8, "outputs": []}, {"metadata": {}, "cell_type": "markdown", "source": "# Model Prediction"}, {"metadata": {"trusted": true}, "cell_type": "code", "source": "def predict(row, model):\n    # convert row to data\n    row = Tensor([row])\n    # make prediction\n    yhat = model(row)\n    # retrieve numpy array\n    yhat = yhat.detach().numpy()\n    return yhat\n \n# prepare the data\npath = 'https://raw.githubusercontent.com/jbrownlee/Datasets/master/ionosphere.csv'\ntrain_dl, test_dl = prepare_data(path)\nprint(len(train_dl.dataset), len(test_dl.dataset))\n# define the network\nmodel = MLP(34)\n# train the model\ntrain_model(train_dl, model)\n# evaluate the model\nacc = evaluate_model(test_dl, model)\nprint('Accuracy: %.3f' % acc)\n# make a single prediction (expect class=1)\nrow = [1,0,0.99539,-0.05889,0.85243,0.02306,0.83398,-0.37708,1,0.03760,0.85243,-0.17755,0.59755,-0.44945,0.60536,-0.38223,0.84356,-0.38542,0.58212,-0.32192,0.56971,-0.29674,0.36946,-0.47357,0.56811,-0.51171,0.41078,-0.46168,0.21266,-0.34090,0.42267,-0.54487,0.18641,-0.45300]\nyhat = predict(row, model)\nprint('Predicted: %.3f (class=%d)' % (yhat, yhat.round()))", "execution_count": 10, "outputs": [{"output_type": "stream", "text": "235 116\nAccuracy: 0.888\nPredicted: 0.996 (class=1)\n", "name": "stdout"}]}, {"metadata": {"trusted": true}, "cell_type": "code", "source": "", "execution_count": null, "outputs": []}], "metadata": {"kernelspec": {"name": "python37464bitd04ad80605dc4165a042c77f86d6bacf", "display_name": "Python 3.7.4 64-bit", "language": "python"}, "language_info": {"name": "python", "version": "3.7.4", "mimetype": "text/x-python", "codemirror_mode": {"name": "ipython", "version": 3}, "pygments_lexer": "ipython3", "nbconvert_exporter": "python", "file_extension": ".py"}}, "nbformat": 4, "nbformat_minor": 4}